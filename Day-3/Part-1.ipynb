{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Our will focus on using Neural Networks to handle tasks related to **Natural Language Processing (NLP)**. \n",
    "\n",
    "- There are many NLP problems that we want computers to be able to solve:\n",
    "\n",
    "# Text Classification\n",
    "\n",
    "* Text classification is a common task where text sequences are categorized.\n",
    "    * **Examples**:\n",
    "        * Classifying e-mails into **spam** or **no-spam**.\n",
    "        * Categorizing news articles into **sport**, **business**, **politics**, etc.\n",
    "\n",
    "* In chatbot development, it's crucial to comprehend user intent. This is termed as **intent classification**.\n",
    "    * **Example**:\n",
    "        * User says: \"How's the weather tomorrow?\"\n",
    "        * Bot understands the intent as: **Check Weather**\n",
    "\n",
    "Note: With intent classification, there can often be a multitude of categories.\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "* **Sentiment analysis** is typically a regression problem. The aim is to assign a numerical value representing the sentiment (how positive or negative) of a sentence.\n",
    "    * **Example**:\n",
    "        * Sentence: \"I love this product!\"\n",
    "        * Sentiment Score: +0.9 (where +1 is very positive and -1 is very negative)\n",
    "\n",
    "* An advanced form is **aspect-based sentiment analysis** (ABSA). Here, sentiment scores are given to different parts of the sentence.\n",
    "    * **Example**:\n",
    "        * Sentence: \"In this restaurant, I liked the cuisine, but the atmosphere was awful.\"\n",
    "        * Sentiments:\n",
    "            * Cuisine: +0.8\n",
    "            * Atmosphere: -0.9\n",
    "\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "\n",
    "* **Named Entity Recognition** (NER) is about extracting specific entities from a given text.\n",
    "    * **Example**:\n",
    "        * Sentence: \"I need to fly to Paris tomorrow.\"\n",
    "        * Extracted Entities:\n",
    "            * Paris: **LOCATION**\n",
    "            * Tomorrow: **DATE**\n",
    "\n",
    "\n",
    "# Keyword Extraction\n",
    "\n",
    "* **Keyword extraction** is akin to NER. However, it focuses on automatically extracting words vital to a sentence's meaning, without any prior training on specific entity types.\n",
    "    * **Example**:\n",
    "        * Sentence: \"The Great Barrier Reef is a natural wonder located off the coast of Australia.\"\n",
    "        * Extracted Keywords: **Great Barrier Reef**, **natural wonder**, **coast**, **Australia**\n",
    "\n",
    "\n",
    "# Text Clustering\n",
    "\n",
    "* **Text clustering** involves grouping similar text sequences or sentences. It can be particularly helpful in contexts like grouping related inquiries in technical support interactions.\n",
    "    * **Example**:\n",
    "        * Tech Support Messages:\n",
    "            * \"My device won't turn on.\"\n",
    "            * \"I can't get my gadget to power up.\"\n",
    "            * \"How do I update my software?\"\n",
    "        * Clustered Results:\n",
    "            * Cluster 1: **Power issues** - \"My device won't turn on.\", \"I can't get my gadget to power up.\"\n",
    "            * Cluster 2: **Software updates** - \"How do I update my software?\"\n",
    "\n",
    "\n",
    "\n",
    "# Question Answering\n",
    "\n",
    "* **Question answering** is about a model's capability to respond to a specific query. Given a text passage and a question, the model identifies the segment of the text containing the answer or, in some cases, generates the answer text.\n",
    "    * **Example**:\n",
    "        * Passage: \"The Eiffel Tower is located in Paris and was completed in 1889.\"\n",
    "        * Question: \"When was the Eiffel Tower completed?\"\n",
    "        * Answer: \"The Eiffel Tower was completed in **1889**.\"\n",
    "\n",
    "\n",
    "\n",
    "# Text Generation\n",
    "\n",
    "* **Text Generation** pertains to a model's ability to produce novel text. It's like a classification task predicting the next character or word based on a given *text prompt*.\n",
    "    * **Example**:\n",
    "        * Prompt: \"Once upon a time,\"\n",
    "        * Generated continuation: \"in a land far away, there lived a wise old dragon.\"\n",
    "\n",
    "* Advanced models, like GPT-3, can tackle other NLP tasks such as classification via techniques like [prompt programming](https://towardsdatascience.com/software-3-0-how-prompting-will-change-the-rules-of-the-game-a982fbfe1e0) or [prompt engineering](https://medium.com/swlh/openai-gpt-3-and-prompt-engineering-dcdc2c5fcd29).\n",
    "\n",
    "\n",
    "\n",
    "# Text Summarization\n",
    "\n",
    "* **Text summarization** is about enabling a computer to \"read\" lengthy text and condense it into a short, coherent summary.\n",
    "    * **Example**:\n",
    "        * Original Text: \"The solar system consists of the Sun and the objects that orbit it. These objects include planets, dwarf planets, moons, and asteroids. The largest planet in the solar system is Jupiter, while Mercury is the smallest.\"\n",
    "        * Summarized Text: \"The solar system includes the Sun, planets, and other celestial objects. Jupiter is the largest planet, and Mercury is the smallest.\"\n",
    "\n",
    "# Machine Translation and NLP\n",
    "\n",
    "* **Machine translation** is a blend of understanding text in one language and generating text in another.\n",
    "    * **Traditional Approach**:\n",
    "        1. Use parsers to convert a sentence into a syntax tree.\n",
    "        2. Extract higher-level semantic structures for the sentence's meaning.\n",
    "        3. Generate the translated output based on this meaning and the target language's grammar.\n",
    "    * **Modern Approach**: Use neural networks for more effective results in many NLP tasks.\n",
    "\n",
    "> Traditionally, many NLP tasks were tackled using methods like grammars. However, the paradigm has shifted towards neural network-based solutions in recent years.\n",
    "\n",
    "* **Resources**:\n",
    "    - Classical methods can be found in the [Natural Language Processing Toolkit (NLTK)](https://www.nltk.org).\n",
    "    - The [NLTK Book](https://www.nltk.org/book/) provides an online guide on solving NLP tasks using NLTK.\n",
    "\n",
    "* **Course Approach**:\n",
    "    * We will predominantly focus on Neural Networks for NLP and incorporate NLTK as required.\n",
    "\n",
    "\n",
    "\n",
    "# Neural Networks: From Images to Text\n",
    "\n",
    "* **Tabular Data & Images**:\n",
    "    - We've explored using neural networks for fixed-size inputs like tabular data and images.\n",
    "    - Images have a predetermined input size.\n",
    "\n",
    "* **Text**:\n",
    "    - Text is a variable-length sequence, making it distinct.\n",
    "    - Textual patterns can be intricate. For instance, the distance between a subject and its negation can vary but should be recognized as a singular pattern.\n",
    "        * **Examples**:\n",
    "            - \"I do not like oranges.\"\n",
    "            - \"I do not like those big colorful tasty oranges.\"\n",
    "\n",
    "* **Solution for Text**:\n",
    "    - Traditional convolutional networks might not capture such complex patterns in text.\n",
    "    - To process language effectively, we introduce new neural architectures:\n",
    "        1. **Recurrent Networks**\n",
    "        2. **Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Overview\n",
    "\n",
    "# What is NLP?\n",
    "\n",
    "- Natural Language Processing (NLP) is a branch of artificial intelligence (AI).\n",
    "- It focuses on the interaction between computers and human language.\n",
    "- NLP enables computers to understand, process, and generate human language as data.\n",
    "\n",
    "# Why NLP Matters\n",
    "\n",
    "- NLP has practical applications in various domains:\n",
    "  - Customer service chatbots\n",
    "  - Sentiment analysis of social media data\n",
    "  - Language translation\n",
    "  - Content generation\n",
    "- It bridges the gap between human communication and AI systems.\n",
    "\n",
    "# NLP Tasks\n",
    "\n",
    "- NLP involves a range of tasks, including:\n",
    "  - Understanding language structure\n",
    "  - Text completion\n",
    "  - Text generation\n",
    "  - Dialogue systems (chatbots)\n",
    "  - Story generation\n",
    "- These tasks have real-world applications and are continuously evolving.\n",
    "\n",
    "\n",
    "\n",
    "## Understanding Language as Data\n",
    "\n",
    "**Tokenization Example**\n",
    "\n",
    "- Tokenization is the process of splitting text into words or phrases.\n",
    "- It's a fundamental step in NLP.\n",
    "\n",
    "**Named Entity Recognition (NER) Demo**\n",
    "\n",
    "- Visit the [spaCy NER Demo](https://explosion.ai/demos/displacy-ent).\n",
    "- Input a sentence with named entities, e.g., \"Apple Inc. is headquartered in Cupertino, California.\"\n",
    "- Click \"Visualize\" to see entity recognition.\n",
    "\n",
    "---\n",
    "\n",
    "## Language Generation Tasks\n",
    "\n",
    "**Text Completion**\n",
    "\n",
    "- Demonstrated by Google Search's auto-suggestion feature.\n",
    "- It suggests the next word or phrase as you type your search query.\n",
    "\n",
    "**Text Generation**\n",
    "\n",
    "- Try the \"GPT-3 Playground\" by OpenAI.\n",
    "- Enter prompts like \"Once upon a time, in a land far, far away...\"\n",
    "- Click \"Create\" to generate creative text.\n",
    "\n",
    "---\n",
    "\n",
    "## Dialogue Systems\n",
    "\n",
    "**Chatbot Demo (Dialogflow)**\n",
    "\n",
    "- Go to [Dialogflow Console](https://console.cloud.google.com/dialogflow).\n",
    "- Create an agent with intents like \"Greeting.\"\n",
    "- Define responses like \"Hello! How can I assist you today?\"\n",
    "- Test your chatbot in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "## Story Generation\n",
    "\n",
    "**Interactive Storytelling (AI Dungeon)**\n",
    "\n",
    "- Visit the [AI Dungeon](https://play.aidungeon.io/) website.\n",
    "- Choose a setting or genre (e.g., \"Fantasy\" or \"Mystery\").\n",
    "- Start a story with a sentence like \"You find yourself in a dark, enchanted forest...\"\n",
    "- AI generates the next part of the story for interactive storytelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text\n",
    "\n",
    "<img src=\"./images/ascii-character-map.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- To solve NLP tasks with neural networks, we need text representation as tensors.\n",
    "- Computers use encodings like ASCII or UTF-8 to map text characters to numbers.\n",
    "- Computers lack inherent understanding, and neural networks must learn meaning during training.\n",
    "- Two common approaches for text representation: character-level and word-level.\n",
    "- Regardless of approach, text is tokenized, converted to numbers, and fed into the network using one-hot encoding.\n",
    "\n",
    "---\n",
    "\n",
    "# N-Grams\n",
    "\n",
    "- Precise word meanings depend on context (e.g., \"neural network\" vs. \"fishing network\").\n",
    "- Address context by considering pairs of words or even tri-grams.\n",
    "- This approach, called n-grams, increases dictionary size.\n",
    "- N-grams can also be used with character-level representation.\n",
    "\n",
    "---\n",
    "\n",
    "# Bag-of-Words and TF/IDF\n",
    "\n",
    "<img src=\"images/bow.png\" width=\"90%\"/>\n",
    "\n",
    "\n",
    "- Text classification requires fixed-size vector representation.\n",
    "- Bag of Words (BoW) combines word representations, often using word frequencies.\n",
    "- BoW can indicate text content based on word frequencies.\n",
    "- TF/IDF (Term Frequency-Inverse Document Frequency) reduces the importance of common words.\n",
    "- TF/IDF considers word frequency across the document collection.\n",
    "\n",
    "---\n",
    "\n",
    "# Semantics of Text\n",
    "\n",
    "- Existing approaches cannot fully capture text semantics.\n",
    "- More powerful neural network models are required.\n",
    "- Explore further in attached notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "list(train_dataset)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because datasets are iterators, if we want to use the data multiple times we need to convert it to list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'said', 'hello']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenizer('He said: hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using vocabulary, we can easily encode out tokenized string into a set of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size if 95810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[599, 3279, 97, 1220, 329, 225, 7368]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "stoi = vocab.get_stoi() # dict to convert tokens to indices\n",
    "\n",
    "def encode(x):\n",
    "    return [stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "encode('I love to play with my words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BoW is a widely used traditional vector representation in text analysis.\n",
    "- Each word is associated with a vector index.\n",
    "- Vector elements store the count of word occurrences in a given document.\n",
    "![Image showing how a bag of words vector representation is represented in memory.](images/bag-of-words-example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "- When training classifiers based on BoW or TF/IDF, we work with high-dimensional one-hot encoded vectors of length `vocab_size`.\n",
    "- One-hot encoding is memory-inefficient and treats words independently, lacking semantic similarity.\n",
    "\n",
    "\n",
    "![Embedding and Semantic Similarity](images/NLP1.webp)\n",
    "\n",
    "- **Embedding** is the idea of representing words as lower-dimensional dense vectors that capture semantic meaning.\n",
    "- It reduces the dimensionality of word vectors.\n",
    "\n",
    "- The embedding layer takes a word as input and produces an output vector of specified `embedding_size`.\n",
    "- Unlike one-hot encoding, it takes a word number as input, avoiding large one-hot-encoded vectors.\n",
    "\n",
    "- Using an embedding layer as the first layer in a classifier network transforms it from a bag-of-words model to an **embedding bag** model.\n",
    "- In this model, words are converted into embeddings, and an aggregate function (e.g., `sum`, `average`, `max`) is applied to these embeddings.\n",
    "\n",
    "![Embedding Classifier Example](images/embedding-classifier-example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "- Early NLP models used word embeddings.\n",
    "- Each word mapped to a fixed-size vector.\n",
    "- Word2Vec, GloVe, and FastText were popular methods.\n",
    "\n",
    "\n",
    "- Word2Vec\n",
    "\n",
    "  - Word2Vec is a popular word embedding technique in natural language processing (NLP).\n",
    "  - It transforms words into dense vectors in a continuous vector space.\n",
    "  - Two main approaches: Skip-gram and Continuous Bag of Words (CBOW).\n",
    "  - Word2Vec captures semantic relationships and context between words.\n",
    "  - Used for various NLP tasks, including text classification, sentiment analysis, and recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/43/21/21f993356303c4c92352d9e7c732f715e86e7b0bc04674be71bb1e9bb05b/gensim-4.3.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from gensim) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from gensim) (1.11.2)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-6.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'learning':\n",
      " [ 7.0887972e-03 -1.5679300e-03  7.9474989e-03 -9.4886590e-03\n",
      " -8.0294991e-03 -6.6403709e-03 -4.0034545e-03  4.9892161e-03\n",
      " -3.8135587e-03 -8.3199050e-03  8.4117772e-03 -3.7470020e-03\n",
      "  8.6086961e-03 -4.8957514e-03  3.9185942e-03  4.9220170e-03\n",
      "  2.3926091e-03 -2.8188038e-03  2.8491246e-03 -8.2562361e-03\n",
      " -2.7655398e-03 -2.5911583e-03  7.2490061e-03 -3.4634031e-03\n",
      " -6.5997029e-03  4.3404270e-03 -4.7448516e-04 -3.5975564e-03\n",
      "  6.8824720e-03  3.8723124e-03 -3.9002013e-03  7.7188847e-04\n",
      "  9.1435025e-03  7.7546560e-03  6.3618720e-03  4.6673026e-03\n",
      "  2.3844899e-03 -1.8416261e-03 -6.3712932e-03 -3.0181051e-04\n",
      " -1.5653884e-03 -5.7228567e-04 -6.2628710e-03  7.4340473e-03\n",
      " -6.5914928e-03 -7.2392775e-03 -2.7571463e-03 -1.5154004e-03\n",
      " -7.6357173e-03  6.9824100e-04 -5.3261113e-03 -1.2755442e-03\n",
      " -7.3651113e-03  1.9605684e-03  3.2731986e-03 -2.3138524e-05\n",
      " -5.4483581e-03 -1.7260861e-03  7.0849168e-03  3.7362587e-03\n",
      " -8.8810492e-03 -3.4135508e-03  2.3541022e-03  2.1380198e-03\n",
      " -9.4640078e-03  4.5711659e-03 -8.6569972e-03 -7.3870681e-03\n",
      "  3.4831120e-03 -3.4709584e-03  3.5644709e-03  8.8940905e-03\n",
      " -3.5743224e-03  9.3204249e-03  1.7110384e-03  9.8477742e-03\n",
      "  5.7050432e-03 -9.1494834e-03 -3.3277308e-03  6.5301750e-03\n",
      "  5.6027793e-03  8.7055154e-03  6.9261026e-03  8.0388878e-03\n",
      " -9.8230084e-03  4.2988253e-03 -5.0300765e-03  3.5123860e-03\n",
      "  6.0566878e-03  4.3921317e-03  7.5123594e-03  1.4977157e-03\n",
      " -1.2649416e-03  5.7684006e-03 -5.6395675e-03  3.8591625e-05\n",
      "  9.4565870e-03 -5.4812501e-03  3.8142789e-03 -8.1130210e-03]\n",
      "Words similar to 'processing':\n",
      " [('natural', 0.06798139214515686), ('awesome', 0.033639226108789444)]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences for training the Word2Vec model\n",
    "sentences = [\n",
    "    [\"machine\", \"learning\", \"is\", \"awesome\"],\n",
    "    [\"word\", \"embeddings\", \"capture\", \"context\"],\n",
    "    [\"natural\", \"language\", \"processing\", \"rocks\"]\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word_vector = model.wv['learning']\n",
    "print(\"Vector for 'learning':\\n\", word_vector)\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = model.wv.most_similar('processing', topn=2)\n",
    "print(\"Words similar to 'processing':\\n\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Limitations of Word Embeddings\n",
    "  - Fixed-length vectors don't capture context well.\n",
    "  - Struggle with polysemy (multiple meanings).\n",
    "  - Limited understanding of word relationships.\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "  - The Transformer architecture revolutionized NLP.\n",
    "  - Introduced self-attention mechanisms.\n",
    "  - Captures context and dependencies effectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Timeline\n",
    "\n",
    "\n",
    "![Embedding Classifier Example](images/NLP2.svg)\n",
    "\n",
    "- June 2017\n",
    "  - Introduction of the Transformer architecture\n",
    "\n",
    "- June 2018\n",
    "  - **GPT** (Generative Pretrained Transformer)\n",
    "  - First pretrained Transformer model\n",
    "  - Used for fine-tuning on NLP tasks\n",
    "  - Achieved state-of-the-art results\n",
    "\n",
    "- October 2018\n",
    "  - **BERT** (Bidirectional Encoder Representations from Transformers)\n",
    "  - Large pretrained model\n",
    "  - Designed for better sentence summarization\n",
    "  - More on this in the next chapter!\n",
    "\n",
    "- February 2019\n",
    "  - **GPT-2**\n",
    "  - Improved and larger version of GPT\n",
    "  - Delayed public release due to ethical concerns\n",
    "\n",
    "- October 2019\n",
    "  - **DistilBERT**\n",
    "  - Distilled version of BERT\n",
    "  - 60% faster, 40% lighter in memory\n",
    "  - Still retains 97% of BERT's performance\n",
    "\n",
    "- **BART and T5**\n",
    "  - Large pretrained models\n",
    "  - Same architecture as the original Transformer\n",
    "\n",
    "- May 2020\n",
    "  - **GPT-3**\n",
    "  - Even bigger than GPT-2\n",
    "  - Performs well on various tasks without fine-tuning\n",
    "  - Known for zero-shot learning\n",
    "\n",
    "# Transformer Model Types\n",
    "\n",
    "- **GPT-like**\n",
    "  - Also known as auto-regressive Transformer models\n",
    "\n",
    "- **BERT-like**\n",
    "  - Also known as auto-encoding Transformer models\n",
    "\n",
    "- **BART/T5-like**\n",
    "  - Also known as sequence-to-sequence Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Transformer Models\n",
    "\n",
    "- All mentioned models (GPT, BERT, BART, T5, etc.) are pretrained language models.\n",
    "- Trained on large amounts of raw text data.\n",
    "- Self-supervised learning: Objective computed automatically from inputs, no human labeling required.\n",
    "\n",
    "- Limitations of Pretrained Models\n",
    "\n",
    "    - Pretrained models have statistical language understanding.\n",
    "    - Not directly useful for specific tasks.\n",
    "    - Require transfer learning for practical applications.\n",
    "\n",
    "- Transfer Learning\n",
    "\n",
    "  - Transfer learning fine-tunes pretrained models for specific tasks.\n",
    "  - Supervised learning with human-annotated labels.\n",
    "  - Improves model performance and adaptability.\n",
    "\n",
    "- Example Task: Causal Language Modeling\n",
    "\n",
    "  - Task: Predict the next word in a sentence given n previous words.\n",
    "  - Output depends on past and present inputs, not future ones.\n",
    "\n",
    "![Embedding Classifier Example](images/NLP3.svg)\n",
    "\n",
    "\n",
    "  - Another example is masked language modeling, in which the model predicts a masked word in the sentence.\n",
    "\n",
    "\n",
    "\n",
    "![Embedding Classifier Example](images/NLP4.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers are big models\n",
    "\n",
    "- Improving performance often involves:\n",
    "  - Increasing model sizes\n",
    "  - Expanding pretrained data\n",
    "\n",
    "![Embedding Classifier Example](images/NLP5.png)\n",
    "\n",
    "- Size vs. Performance\n",
    "  - Larger models tend to perform better.\n",
    "  - But training large models is resource-intensive.\n",
    "\n",
    "\n",
    "- Environmental Impact\n",
    "\n",
    "  - Large models have a significant carbon footprint.\n",
    "  - Time, compute resources, and environmental costs.\n",
    "  - Even efforts to reduce impact still result in substantial costs.\n",
    "\n",
    "\n",
    "\n",
    "- The Need for Sharing\n",
    "  - Sharing pretrained models is crucial.\n",
    "  - Reduces overall compute cost and carbon footprint.\n",
    "  - Benefits research teams, student organizations, and companies.\n",
    "\n",
    "---\n",
    "\n",
    "# Evaluate Carbon Footprint\n",
    "\n",
    "- Tools available to evaluate carbon footprint:\n",
    "  - ML CO2 Impact\n",
    "  - Code Carbon (integrated in 🤗 Transformers)\n",
    "  \n",
    "![Embedding Classifier Example](images/NLP6.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining vs. Fine-Tuning\n",
    "\n",
    "- Pretraining\n",
    "    - Pretraining uses a large corpus of data.\n",
    "    - Training can take several weeks.\n",
    "\n",
    "\n",
    "![Embedding Classifier Example](images/NLP7.svg)\n",
    "\n",
    "- Fine-Tuning\n",
    "  - Fine-tuning occurs after pretraining.\n",
    "  - Utilizes a pretrained language model.\n",
    "  - Additional training with a task-specific dataset.\n",
    "\n",
    "\n",
    "![Embedding Classifier Example](images/NLP8.svg)\n",
    "\n",
    "- Why Not Direct Training?\n",
    "  - Pretrained models have some similarities with fine-tuning data.\n",
    "  - Fine-tuning leverages knowledge from pretraining.\n",
    "  - Requires less data, time, and resources.\n",
    "\n",
    "- Example Scenario\n",
    "  - Pretrained model in English.\n",
    "  - Fine-tuning on arXiv corpus.\n",
    "  - Creates a science/research-based model.\n",
    "  - Limited data needed for fine-tuning.\n",
    "\n",
    "- Transfer Learning\n",
    "\n",
    "  - Knowledge from pretraining \"transferred\" to fine-tuning.\n",
    "  - Effective way to adapt pretrained models to specific tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General architecture\n",
    "\n",
    "- Two main components:\n",
    "  - Autoencoders\n",
    "  - Attention layers\n",
    "\n",
    "#### Autoencoders\n",
    "\n",
    "![Transformer Architecture](images/NLP10.svg)\n",
    "\n",
    "#### Attention Layers\n",
    "\n",
    "- Attention layers instruct the model to focus on specific words in a sentence while processing the representation of each word.\n",
    "- They help the model pay attention to certain words while ignoring others.\n",
    "- In the context of translation, attention layers are crucial because they allow the model to consider adjacent words for proper translation.\n",
    "- For example, when translating from English to French, attention is needed for subjects and gender agreement.\n",
    "- Attention layers ensure that words' meanings are deeply influenced by their surrounding context.\n",
    "- They are essential for handling complex sentences and grammar rules in natural language processing tasks.\n",
    "- Understanding attention layers is a foundation for comprehending the Transformer architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer\n",
    "\n",
    "![Transformer Architecture](images/NLP9.svg)\n",
    "\n",
    "- Transformer architecture designed for translation.\n",
    "- Encoder processes inputs (sentences) in one language.\n",
    "- Decoder generates translations in the target language.\n",
    "\n",
    "-  Attention Mechanism in Encoder\n",
    "   - Encoder uses attention layers.\n",
    "   - Can attend to all words in a sentence.\n",
    "   - Considers both preceding and following words.\n",
    "  \n",
    "- Attention Mechanism in Decoder\n",
    "\n",
    "  - Decoder works sequentially.\n",
    "  - Processes words one by one.\n",
    "  - Limited to using words before the current word.\n",
    "\n",
    "- Training Speed-up\n",
    "\n",
    "  - During training, the decoder sees the entire target sentence.\n",
    "  - It can't use future words for prediction.\n",
    "  - For example, when predicting the fourth word, it only has access to words 1 to 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 15.8MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 90.9kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 1.66MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [00:55<00:00, 7.97MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings Shape: torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"Hello, how are you doing today?\"\n",
    "\n",
    "# Tokenize input text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Forward pass through BERT model\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Get the output embeddings (contextualized word representations)\n",
    "word_embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Print the shape of the word embeddings\n",
    "print(\"Word Embeddings Shape:\", word_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Text Classification \n",
    "\n",
    "\n",
    "\n",
    "## Step 1: Load Pre-trained BERT\n",
    "\n",
    "- Import necessary libraries.\n",
    "- Load pre-trained BERT tokenizer and model (e.g., `bert-base-uncased`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 for binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Prepare Training Data\n",
    "\n",
    "- Define sample training data and labels.\n",
    "- Tokenize and encode the training data using the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data\n",
    "texts = [\"This is a positive review.\", \"This is a negative review.\"]\n",
    "labels = [1, 0]  # 1 for positive, 0 for negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create DataLoader\n",
    "\n",
    "- Create a DataLoader for efficient training.\n",
    "- Set batch size and shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize and encode the training data\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in texts:\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids.append(encoded_text['input_ids'])\n",
    "    attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create a DataLoader for training data\n",
    "batch_size = 2\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Fine-Tune BERT\n",
    "\n",
    "- Set up optimization (e.g., AdamW) and training parameters.\n",
    "- Iterate through epochs, compute loss, and update model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 0.7251\n",
      "Epoch 2/3, Average Loss: 0.6729\n",
      "Epoch 3/3, Average Loss: 0.6761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fine-tuning BERT on the classification task (you can adjust the number of training epochs)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, label = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Evaluation\n",
    "\n",
    "- Switch to evaluation mode.\n",
    "- Tokenize and encode sample test data.\n",
    "- Make predictions and calculate classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: tensor([1, 1])\n",
      "True Labels: [1 0]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "test_texts = [\"This movie is great.\", \"I didn't like the film.\"]\n",
    "true_labels = [1, 0]\n",
    "\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for text in test_texts:\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    test_input_ids.append(encoded_text['input_ids'])\n",
    "    test_attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "true_labels = torch.tensor(true_labels)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(test_input_ids, attention_mask=test_attention_masks).logits\n",
    "\n",
    "predicted_labels = np.argmax(logits, axis=1)\n",
    "print(\"Predicted Labels:\", predicted_labels)\n",
    "print(\"True Labels:\", true_labels.numpy())\n",
    "\n",
    "# Classification report and accuracy\n",
    "classification_rep = classification_report(true_labels.numpy(), predicted_labels)\n",
    "accuracy = accuracy_score(true_labels.numpy(), predicted_labels)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "labels = ['Negative', 'Positive']\n",
    "y_pos = np.arange(len(labels))\n",
    "scores = [logits[0][0].item(), logits[1][1].item()]\n",
    "ax.bar(y_pos, scores, align='center', alpha=0.5)\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_xticks(y_pos)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_title('BERT Sentiment Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 656/656 [00:00<00:00, 2.62MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 479M/479M [01:00<00:00, 7.97MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 74.0/74.0 [00:00<00:00, 291kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 816k/816k [00:00<00:00, 15.5MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 458k/458k [00:00<00:00, 11.5MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.27M/1.27M [00:00<00:00, 23.5MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello! I am a neural network, and I want to say that it is a pleasure to meet you. we have heard a wonderful thing about our friend\\'s work. i am so delighted to hear it, but our understanding is that the neural network was created in the time of the last great depression : there was a new generation of neural networks, and the neural network was created. \" \\n \" what you speak is,\\'i think we can make it.\\'but it means,\\'how'},\n",
       " {'generated_text': 'Hello! I am a neural network, and I want to say that you look beautiful. you are a great person. do you see anything you \\'d like to do today? \" \\n she looked down, and she tried to smile. \\n \" yes, i can see how this makes you feel, but it just seems so wrong, the way you are sitting there. you are like a sculpture that has come up and is in a state of disrepair, and if you could please just take off your'},\n",
       " {'generated_text': 'Hello! I am a neural network, and I want to say that they are just a shell, like a human. the computer, the computer! \" \" \\n \" why don\\'t you try us, \" asked one of the alien warriors. \" we\\'re going to help you, \" the other replied, \" if you wish to join us. \" \\n \" what do you mean? \" asked disappointedly. \" we are no more like animals than we were before! and besides, all'},\n",
       " {'generated_text': 'Hello! I am a neural network, and I want to say that we\\'re sorry for the intrusion, it\\'s not our intention to harm you. please, follow these three down. we believe that a direct order from the president here won\\'t be tolerated. please have a seat. \" \\n at first, all they saw was the head of their security, a man with a face resembling an anteater\\'s. then it became clear, however, that he was a high - pitched, reedy voice'},\n",
       " {'generated_text': 'Hello! I am a neural network, and I want to say that i never understood much about you guys, but all that happens is i\\'ve been a hacker for as long as i can remember. but i need to tell you a few things first. as a matter of fact, i may have a few things to share with you about people. so, let\\'s move on to the meeting... \" \\n chapter twenty seven \\n we all sit around the table, discussing our new roles in the project'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = 'openai-gpt' \n",
    "\n",
    "generator = pipeline('text-generation', model=model_name)\n",
    "\n",
    "generator(\"Hello! I am a neural network, and I want to say that\", max_length=100, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Synonyms of a word cat: \" no. \" i wondered what other words were going through'},\n",
       " {'generated_text': \"Synonyms of a word cat: which was what the cat's name was, but that was\"},\n",
       " {'generated_text': 'Synonyms of a word cat: \" human, cat and more... how do you say it'},\n",
       " {'generated_text': \"Synonyms of a word cat: the dog which had been the dog of the previous night's\"},\n",
       " {'generated_text': 'Synonyms of a word cat: to get rid of this damned cat. \\n the smell of'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Synonyms of a word cat:\", max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I love when you say this -> Positive\\nI have myself -> Negative\\nThis is awful for you to say this -> negative this happens to be the world of you - > positive the next day'},\n",
       " {'generated_text': 'I love when you say this -> Positive\\nI have myself -> Negative\\nThis is awful for you to say this -> negative i can not even - > negative is my most favorite - > positive'},\n",
       " {'generated_text': 'I love when you say this -> Positive\\nI have myself -> Negative\\nThis is awful for you to say this -> positive i have the opposite negative, positive i have a very positive reaction to'},\n",
       " {'generated_text': \"I love when you say this -> Positive\\nI have myself -> Negative\\nThis is awful for you to say this -> positive this is a - and - so you can't think? \\n and\"},\n",
       " {'generated_text': 'I love when you say this -> Positive\\nI have myself -> Negative\\nThis is awful for you to say this -> negative this is a bad thing for you to say - > positive \\n the'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"I love when you say this -> Positive\\nI have myself -> Negative\\nThis is awful for you to say this ->\", max_length=40, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Translate English to French: cat => chat, dog => chien, student =>  game between the french and americans : chess, games,'},\n",
       " {'generated_text': 'Translate English to French: cat => chat, dog => chien, student =>  cat = > cat = > feline, teacher = >'},\n",
       " {'generated_text': \"Translate English to French: cat => chat, dog => chien, student =>  friend, \\n '... and in writing this, as\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Translate English to French: cat => chat, dog => chien, student => \", top_k=50, max_length=30, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'People who liked the movie The Matrix also liked  the princess bride. which was why the story was just starting to come together. \\n \" you\\'re right, \" i said. \" we are the most'},\n",
       " {'generated_text': \"People who liked the movie The Matrix also liked  the old movies. \\n in spite of those issues, both girls were interested in the sci - fi movies. that's what the new book was for.\"},\n",
       " {'generated_text': \"People who liked the movie The Matrix also liked  a lot more than the movies the guy in line - that meant that even if someone started going to movies without it - he wouldn't understand the story or\"},\n",
       " {'generated_text': 'People who liked the movie The Matrix also liked  it the movie of the new york yankees. they even took on more personality - wise than my friends. \\n i was having a major meltdown over this one'},\n",
       " {'generated_text': 'People who liked the movie The Matrix also liked  her movies. she had been too busy running around and writing her novel to spend time with these characters. they were the ones who had done the mystery movie'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"People who liked the movie The Matrix also liked \", max_length=40, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Sampling Strategies\n",
    "\n",
    "So far we have been using simple **greedy** sampling strategy, when we selected next word based on the highest probability. Here is how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw i had a large bouquet of roses on the table beside me. i picked up the flowers, took a long look over them and immediately dropped them. \\n \" what\\'s wrong, sweetheart? \" luke asked as he walked over and knelt down in front of me. \\n \" nothing. i was just so shocked. \" \\n \" you\\'re'},\n",
       " {'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw the most beautiful woman i had ever seen. the one person i truly thought i loved. she was sitting on a large black leather couch of what appeared to be leather seats. she had the sexiest face i have ever seen in my life and she had a red satin bow on her head, just like the dress. she was so lovely.'},\n",
       " {'generated_text': \"It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw that someone had just entered. there was a bed and a table by the window. a large man wearing a black, leather hood was walking towards the window. the window was full of pictures in black frames. the one who left was the man who 'd left me the house yesterday. i have a feeling that there's been a change,\"},\n",
       " {'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw i was the only waiter with a name tag identified as bob. i had just stepped into the office of michael eddy who was sitting at the desk when i heard him announce, \" hey, bob, what\\'s this? \" \\n \" good evening, michael. my name is bob bob. \" \\n \" hi, i\\'m eddy. \"'},\n",
       " {'generated_text': \"It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw a young woman - it was a young woman - the same type that i saw the other days of my day. \\n i moved out of the way, though. there's a young woman in here, i would guess about your age - a cute brunette, but a little too pretty to be my type. i walked over to her.\"}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw\"\n",
    "generator(prompt,max_length=100,num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beam Search** allows the generator to explore several directions (*beams*) of text generation, and select the ones with highers overall score. You can do beam search by providing `num_beams` parameter. You can also specify `no_repeat_ngram_size` to penalize the model for repeating n-grams of a given size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw a woman sitting at a desk. she looked up at me, smiled, and said, \" good evening. \" \\n i sat down in the chair across from her. \\n \" what can i do for you? \" she asked in a soft voice, as she leaned back in her chair and put her feet up on the desk in front'},\n",
       " {'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw a woman sitting at a desk. she looked up at me, smiled, and said, \" good evening. \" \\n i sat down in the chair across from her. \\n \" what can i do for you? \" she asked in a soft voice, as she leaned back in her chair and put her feet up on the desk, crossing'},\n",
       " {'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw a woman sitting at a desk. she looked up at me, smiled, and said, \" good evening. \" \\n i sat down in the chair across from her. \\n \" what can i do for you? \" she asked in a soft voice, as she leaned back in her chair and crossed one leg over the other. her hair'},\n",
       " {'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw a woman sitting at a desk. she looked up at me, smiled, and said, \" good evening. \" \\n i sat down in the chair across from her. \\n \" what can i do for you? \" she asked in a soft voice, as she leaned back in her chair and crossed one leg over the other. her long'},\n",
       " {'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw a woman sitting at a desk. she looked up at me, smiled, and said, \" good evening. \" \\n i sat down in the chair across from her. \\n \" what can i do for you? \" she asked in a soft voice, as she leaned back in her chair and crossed her arms over her chest. her eyes'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw\"\n",
    "generator(prompt,max_length=100,num_return_sequences=5,num_beams=10,no_repeat_ngram_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling** selects the next word non-deterministically, using the probability distribution returned by the model. You turn on sampling using `do_sample=True` parameter. You can also specify `temperature`, to make the model more or less deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw her. she was on the bed, but she looked very different. \\n \" honey, what\\'s the matter? \" i asked. \\n she sat up. \" i can\\'t believe it\\'s real. i\\'ve been dreaming about you for the last two days. \" \\n \" i can\\'t believe it either. i guess that\\'s how'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"It was early evening when I can back from work. I usually work late, but this time it was an exception. When I entered a room, I saw\"\n",
    "generator(prompt,max_length=100,do_sample=True,temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide to additional parameters to sampling:\n",
    "* `top_k` specifies the number of word options to consider when using sampling. This minimizes the chance of getting weird (low-probability) words in our text.\n",
    "* `top_p` is similar, but we chose the smallest subset of most probable words, whose total probability is larger than p.\n",
    "\n",
    "Feel free to experiment with adding those parameters in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
