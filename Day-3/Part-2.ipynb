{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Project: Text Classification with DBpedia Dataset\n",
    "\n",
    "### Description:\n",
    "In this project, you will be working with the DBpedia ontology dataset. The task is to classify textual descriptions into one of 14 classes such as \"Company\", \"Artist\", \"Athlete\", and so forth. This classification problem will allow you to apply and solidify your understanding of NLP and machine learning concepts in a hands-on manner.\n",
    "\n",
    "### Objective:\n",
    "- Understand and process a real-world dataset.\n",
    "- Implement a text classification model using the GPT-2 architecture.\n",
    "- Evaluate the performance of your model and aim to achieve the highest accuracy possible.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "#### 1. Dataset Exploration \n",
    "- Load the `dbpedia_14` dataset.\n",
    "- Analyze the dataset: understand the distribution of labels, the length of textual descriptions, etc.\n",
    "- Split the dataset into training and test sets.\n",
    "\n",
    "#### 2. Data Pre-processing \n",
    "- Tokenize the textual descriptions.\n",
    "- Ensure that your data is in the appropriate format for model training (e.g., tensors).\n",
    "\n",
    "#### 3. Model Building \n",
    "- Define a classification model using the GPT-2 architecture.\n",
    "- Implement the forward pass.\n",
    "- Choose an appropriate loss function for multi-class classification.\n",
    "\n",
    "#### 4. Model Training \n",
    "- Implement a training loop.\n",
    "- Make sure to track the loss over time. This will help you understand if your model is learning.\n",
    "- If time permits, play around with hyperparameters to see if you can get better results.\n",
    "\n",
    "#### 5. Model Evaluation\n",
    "- Evaluate your model on the test dataset.\n",
    "- Compute classification metrics such as accuracy, F1 score, etc.\n",
    "- (Optional) Analyze cases where your model fails. What can you infer from these mistakes?\n",
    "\n",
    "#### 6. Discussion & Reflection \n",
    "- Share your results with the class.\n",
    "- Reflect on what you've learned: challenges faced, insights gained, and potential improvements.\n",
    "\n",
    "\n",
    "### Deliverables:\n",
    "1. A Jupyter Notebook or Python script with:\n",
    "    - Code for each task.\n",
    "    - Comments explaining your thought process.\n",
    "    - Results and visualizations (if any).\n",
    "2. A brief report summarizing your findings, reflecting on your model's performance, and suggesting potential improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Task 1: Dataset Exploration\n",
    "# Load the dbpedia_14 dataset\n",
    "dataset = load_dataset('dbpedia_14')\n",
    "\n",
    "# Quick exploration\n",
    "print(dataset['train'].shape)\n",
    "print(dataset['train'].features)\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# Task 2: Data Pre-processing\n",
    "# Tokenize the textual descriptions\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_and_format(examples):\n",
    "    encodings = tokenizer(examples['content'], truncation=True, padding='max_length', max_length=256)\n",
    "    encodings['labels'] = examples['label']\n",
    "    return encodings\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_format, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=8)\n",
    "test_dataloader = DataLoader(tokenized_datasets['test'], batch_size=8)\n",
    "\n",
    "# Task 3: Model Building\n",
    "class GPT2ForClassification(nn.Module):\n",
    "    def __init__(self, num_labels=14):\n",
    "        super(GPT2ForClassification, self).__init__()\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2-medium')\n",
    "        self.classifier = nn.Linear(self.gpt2.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logits = self.classifier(hidden_states[:, -1])\n",
    "        return logits\n",
    "\n",
    "model = GPT2ForClassification().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Task 4: Model Training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 2  # Sample value. Can be increased as needed.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs, masks, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "        \n",
    "        logits = model(inputs, masks)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed!\")\n",
    "\n",
    "# Task 5: Model Evaluation\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs, masks, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "        logits = model(inputs, masks)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        \n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute accuracy and F1 score\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Task 6: Discussion & Reflection is more about sharing and understanding the results, so no code is provided for that.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
