{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction to GAI and its application\n",
    "- Generative Model Types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI: A Year of Transformation\n",
    "\n",
    "<img src=\"./images/GAI1.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "- Generative AI has gained extensive attention and investment in the past year.\n",
    "  - It can produce coherent text, images, code, and beyond-impressive outputs with just a simple textual prompt.\n",
    "- Generative AI goes beyond typical natural language processing (NLP) tasks.\n",
    "\n",
    "- Countless use cases:\n",
    "  - Explaining complex algorithms.\n",
    "  - Building bots.\n",
    "  - Assisting in app development.\n",
    "  - Explaining academic concepts.\n",
    "\n",
    "- Fields undergoing transformation:\n",
    "  - Animation.\n",
    "  - Gaming.\n",
    "  - Art.\n",
    "  - Movies.\n",
    "  - Architecture.\n",
    "  - Coffee Industry (?)\n",
    "\n",
    "#### The \"Aha!\" Moment in 2022\n",
    "\n",
    "- Important questions:\n",
    "  - Why now?\n",
    "  - What's next?\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Generative AI?\n",
    "\n",
    "<img src=\"./images/GAI8.webp\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- Generative AI is a subfield of **machine learning**.\n",
    "- It involves training AI models on **real-world data**.\n",
    "- These models generate new content like text, images, and code.\n",
    "- Comparable to **what humans would create**.\n",
    "\n",
    "\n",
    "#### How Generative AI Works\n",
    "\n",
    "- Training algorithms on large datasets.\n",
    "  - Identifying and learning patterns.\n",
    "    - **Neural networks** learn these patterns.\n",
    "      - **Generate new data** following the learned patterns.\n",
    "\n",
    "\n",
    "#### Generative AI in Natural Language Processing (NLP)\n",
    "\n",
    "- Generative AI in NLP processes a vast corpus.\n",
    "  - Responds to prompts based on learned probabilities.\n",
    "    - Examples: Autocomplete and advanced models like ChatGPT and DALL-E.\n",
    "  - Utilizes different model architectures.\n",
    "\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "#  General Components for Generative AI\n",
    "\n",
    "1. **Input Data:**\n",
    "   - Generative AI models start with input data, which can be in various forms such as text, images, or structured data.\n",
    "\n",
    "2. **Encoder (Optional):**\n",
    "   - Some models use an encoder to transform input data into a suitable representation, especially in sequence-to-sequence models.\n",
    "\n",
    "3. **Generator (Decoder):**\n",
    "   - The core of the generative model is the generator or decoder, which generates new data based on learned patterns.\n",
    "   - This component typically consists of neural network layers.\n",
    "\n",
    "4. **Latent Space (Optional):**\n",
    "   - In certain models like VAEs and GANs, a latent space is used to represent data in a compressed form.\n",
    "   - The latent space is learned during training and can be sampled for generating new data.\n",
    "\n",
    "5. **Loss Function:**\n",
    "   - Generative models use a loss function to measure the difference between generated data and target data.\n",
    "   - The model aims to minimize this loss during training to improve its generative capabilities.\n",
    "\n",
    "6. **Training Data:**\n",
    "   - Generative models are trained on a dataset containing examples of the data they are supposed to generate.\n",
    "   - The model learns from this data to capture underlying patterns.\n",
    "\n",
    "7. **Optimizer:**\n",
    "   - Optimization algorithms like SGD or Adam are used to update the model's parameters during training to minimize the loss function.\n",
    "\n",
    "8. **Sampling:**\n",
    "   - Once trained, the model can generate new data by sampling from the learned distribution in the latent space or directly from the generator.\n",
    "\n",
    "9. **Output Data:**\n",
    "   - The generated data is the final output of the generative AI model, and it can be in the same or a different format as the input data, depending on the task.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History of Generative AI\n",
    "\n",
    "#### Early Generative AI - Eliza\n",
    "\n",
    "- Eliza chatbot developed in 1966 by Joseph Weizenbaum.\n",
    "- Early implementations used rules-based approaches.\n",
    "- Eliza had a limited vocabulary, lacked context, and overrelied on patterns.\n",
    "- These limitations led to frequent breakdowns.\n",
    "- **Challenges**\n",
    "  - Customization and expansion of early chatbots were challenging\n",
    "  - They struggled to adapt to different user inputs.\n",
    "  - Lack of context made meaningful conversations difficult.\n",
    "  - These limitations hindered their practical use.\n",
    "\n",
    "<img src=\"./images/ELIZA_conversation.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "#### Recent Progress in Generative AI\n",
    "\n",
    "- Key Factors in Recent Success\n",
    "  - Deep learning's three critical components:\n",
    "    - **Scaling models**: Larger and more complex architectures.\n",
    "    - **Large datasets**: Abundant real-world data for training.\n",
    "    - Increased **compute power**: Faster training and more sophisticated models.\n",
    "- These factors work together to drive the generative AI revolution.\n",
    "\n",
    "<img src=\"./images/whynow.png\" width=\"800\" align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPUs and their Application to Machine Learning\n",
    "-  GPUs vs. CPUs\n",
    "   - GPUs designed for parallel processing.\n",
    "   - Ideal for computationally intensive tasks.\n",
    "   - Thousands of smaller cores for simultaneous processing.\n",
    "   - Contrasts with CPUs focused on sequential processing.\n",
    "\n",
    "- GPUs excel in training deep neural networks.\n",
    "- **Parallelism** speeds up training significantly.\n",
    "- Enables the handling of large, complex networks.\n",
    "\n",
    "#### Beyond Training\n",
    "\n",
    "- GPUs not limited to training.\n",
    "- Used for inference and real-time applications.\n",
    "- Widely adopted in industries like healthcare, finance, and gaming.\n",
    "\n",
    "<img src=\"./images/Nvidia_CUDA_Logo.jpg\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet — 2012 — The Deep Learning Revolution\n",
    "- Deep learning and CNNs led the charge.\n",
    "- Particularly effective in solving computer vision problems.\n",
    "- CNNs (Convolutional Neural Networks) existed since the 1990s.\n",
    "  - Layers in a CNN\n",
    "    - Convolutional layers\n",
    "      - filters to detect various features.\n",
    "    - Pooling layers\n",
    "      - Reduce the spatial dimensions to preserve important information.\n",
    "    - Fully connected layers\n",
    "      - Often used for classification\n",
    "\n",
    "\n",
    "<img src=\"./images/CNN.jpeg\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- Previously impractical due to intensive computing requirements.\n",
    "\n",
    "- https://poloclub.github.io/cnn-explainer/\n",
    "\n",
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>\n",
    "\n",
    "## ImageNet\n",
    "\n",
    "- In 2009, Stanford AI researchers introduced ImageNet.\n",
    "  - A labeled image dataset for training computer vision algorithms.\n",
    "\n",
    "**The Breakthrough in 2012**\n",
    "\n",
    "- In 2012, AlexNet emerged.\n",
    "  - A CNN model trained on GPUs and ImageNet data.\n",
    "    - An astonishing 11% performance gap with the runner-up!\n",
    "- CNNs became vital for various AI tasks.\n",
    "- Computer vision advancements drove AI's broader adoption.\n",
    "- AI transformed industries like healthcare and autonomous driving.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Attention Is All You Need (Google) — 2017\n",
    "\n",
    "- Deep learning lacked in natural language processing (NLP).\n",
    "- NLP not just about translation or classification.\n",
    "- The challenge was coherent conversations with humans.\n",
    "\n",
    "#### RNNs and LSTMs\n",
    "\n",
    "\n",
    "<img src=\"./images/RNNLSTM.png\"  width=\"400\" align=\"center\"/>\n",
    "\n",
    "- Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks were early staples in natural language processing and time series analysis.\n",
    "- RNNs, with their sequential nature, were promising for handling sequences of data. \n",
    "- LSTMs, a specialized variant of RNNs, showed better performance in capturing long-range dependencies. \n",
    "- However, both models had **limitations when it came to processing extensive textual data**.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "- RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory) were the go-to models for time-based data.\n",
    "- Proficient at short sequences but struggled with **longer text**.\n",
    "- Couldn't capture complex ideas in extended text.\n",
    "\n",
    "#### The Birth of Transformers\n",
    "\n",
    "- Google introduced the \"Transformer\" model in 2017.\n",
    "- Presented in the groundbreaking paper \"Attention Is All You Need.\"\n",
    "- A milestone that revolutionized translation problems.\n",
    "\n",
    "\n",
    "#### The Power of Attention\n",
    "\n",
    "- \"Attention\" mechanism - a neural network game-changer.\n",
    "- Allows analyzing the entire input sequence.\n",
    "- Determines relevance to each component of the output.\n",
    "- Transforms NLP and many other AI domains.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/06/3817f9bb923437ead9a794f0ac0d03b8b5e0478ab112db4c413dd37c09da/transformers-4.33.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.15.1 from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/96/06/4beb652c0fe16834032e54f0956443d4cc797fe645527acee59e7deaa0a2/PyYAML-6.0.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/6b/20/8a419181449227182d61908484477d23d01b2b35211a45e838b746da8bb4/regex-2023.8.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/b9/8a/5de008c8a2ba7b2e788714be6d8a954b55fb769154ce3a0a76c9d6e79552/safetensors-0.3.3-cp310-cp310-macosx_10_11_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-macosx_10_11_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.15.1->transformers)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/6a/af/c673e8c663e17bd4fb201a6f029153ad5d7023aa4442d81c7987743db379/fsspec-2023.9.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-macosx_10_9_x86_64.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.8.8-cp310-cp310-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.3-cp310-cp310-macosx_10_11_x86_64.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.2/404.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, safetensors, regex, pyyaml, fsspec, huggingface-hub, transformers\n",
      "Successfully installed fsspec-2023.9.1 huggingface-hub-0.17.2 pyyaml-6.0.1 regex-2023.8.8 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "There are used to illustrate traditional RNNs. RNNs. RNNs. RNNs. RNNs. RNNs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Sample text data\n",
    "text = \"There are example texts. It's used to illustrate traditional RNNs.\"\n",
    "\n",
    "# Preprocess the text and create sequences\n",
    "tokens = text.split()\n",
    "word_to_idx = {word: idx for idx, word in enumerate(tokens)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "seq_length = 5\n",
    "\n",
    "data = []\n",
    "for i in range(len(tokens) - seq_length):\n",
    "    seq_in = tokens[i:i + seq_length]\n",
    "    seq_out = tokens[i + seq_length]\n",
    "    data.append((seq_in, seq_out))\n",
    "\n",
    "# Define an RNN-based text generation model\n",
    "class RNNTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNNTextGenerator, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embeddings(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(tokens)\n",
    "embedding_dim = 10\n",
    "hidden_dim = 50\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Create and train the RNN model\n",
    "model_rnn = RNNTextGenerator(vocab_size, embedding_dim, hidden_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for seq_in, seq_out in data:\n",
    "        seq_in_idx = torch.tensor([word_to_idx[word] for word in seq_in], dtype=torch.long)\n",
    "        seq_out_idx = torch.tensor([word_to_idx[seq_out]], dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden = None\n",
    "        for i in range(seq_length):\n",
    "            output, hidden = model_rnn(seq_in_idx[i].view(1, -1), hidden)\n",
    "        \n",
    "        loss = criterion(output.view(1, -1), seq_out_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Generate text using the RNN model\n",
    "seed_text = \"There are\"\n",
    "predicted_text = seed_text\n",
    "hidden = None\n",
    "for _ in range(10):\n",
    "    seq_in_idx = torch.tensor([word_to_idx[word] for word in seed_text.split()], dtype=torch.long)\n",
    "    output, hidden = model_rnn(seq_in_idx[-1].view(1, -1), hidden)\n",
    "    predicted_word_idx = torch.argmax(output).item()\n",
    "    predicted_word = idx_to_word[predicted_word_idx]\n",
    "    predicted_text += \" \" + predicted_word\n",
    "    seed_text += \" \" + predicted_word\n",
    "print(\"********\")\n",
    "print(predicted_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mzihayat/VSC/Seshat/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 677kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 548M/548M [01:08<00:00, 7.96MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 579kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 14.4MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 14.9MB/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "There are many ways to get around the law.\n",
      "\n",
      "The first is to get a license.\n",
      "\n",
      "The second is to get a license from\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Generate text using the Transformer-based model\n",
    "input_text = \"There are\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output = model.generate(input_ids, max_length=30, num_return_sequences=1)\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"********\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers Beyond Translation\n",
    "\n",
    "- Transformers excelled in translation and beyond.\n",
    "- State-of-the-art models for numerous NLP tasks.\n",
    "- Recently, Transformers made waves in computer vision.\n",
    "\n",
    "#### Transformer's Impact on NLP\n",
    "\n",
    "- Transformers reshaped NLP landscape.\n",
    "- Fostered advancements in conversational AI.\n",
    "- Enabled applications in chatbots, virtual assistants, and more.\n",
    "- Human-computer interactions reached new heights.\n",
    "\n",
    "\n",
    "<img src=\"./images/NLPEvolution.png\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction, Scale, and Fine Tuning — BERT (Google) and GPT (OpenAI) Family — 2018\n",
    "\n",
    "- AI needed to understand **language beyond translation**.\n",
    "- Conversational AI and contextual understanding.\n",
    "- BERT and GPT addressed this crucial gap.\n",
    "\n",
    "#### Introducing BERT\n",
    "\n",
    "- BERT (Bidirectional Encoder Representations from Transformers).\n",
    "- Google's approach to contextual language understanding.\n",
    "- Trained on vast amounts of text to predict missing words.\n",
    "- BERT's Impact\n",
    "  - Achieved remarkable results in sentiment analysis, question answering, and more.\n",
    "  - Contextual embeddings revolutionized language understanding.\n",
    "\n",
    "#### GPT - A Different Approach\n",
    "\n",
    "- GPT (Generative Pre-trained Transformer) by OpenAI.\n",
    "- Focus on autoregressive language modeling.\n",
    "  - Learning to generate text one word at a time.\n",
    "- GPT's Language Generation\n",
    "  - GPT-2's surprising ability to generate coherent text.\n",
    "  - Human-like responses in chatbots and text generation.\n",
    "  - Demonstrated the power of pre-trained models.\n",
    "\n",
    "\n",
    "<img src=\"./images/GPT.jpeg\" width=\"500\" align=\"center\"/>\n",
    "---\n",
    "\n",
    "#### Scaling Challenges\n",
    "\n",
    "- Collecting quality training data remained a challenge.\n",
    "- ImageNet required meticulous labeling of thousands of images.\n",
    "- Text datasets for language tasks were equally demanding.\n",
    "\n",
    "#### GPT-3: Scaling New Heights\n",
    "\n",
    "- OpenAI introduced GPT-3 with 175 billion parameters.\n",
    "- The largest and most powerful language model to date.\n",
    "\n",
    "#### Fine Tuning - Customizing Models\n",
    "\n",
    "- Fine tuning adapts large models to specific tasks.\n",
    "- Cost-effective compared to training from scratch.\n",
    "- Application in fields like healthcare, finance, and more.\n",
    "- Examples\n",
    "  - Fine-tuned models for medical document processing.\n",
    "  - Improved accuracy in identifying medical conditions.\n",
    "  - OpenAI's partnership with Microsoft for domain-specific AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with and without BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, pipeline\n\u001b[1;32m      6\u001b[0m \u001b[39m# Load the dataset from the CSV file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mmovie_reviews.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[1;32m     10\u001b[0m X \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "df = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Traditional Approach: TF-IDF + Logistic Regression\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "sample_review = [\"The movie was disappointing. The acting was mediocre, and the plot lacked depth. I would not recommend it.\"]\n",
    "\n",
    "# Transform the sample review using TF-IDF\n",
    "sample_review_tfidf = tfidf_vectorizer.transform(sample_review)\n",
    "\n",
    "# Predict the sentiment for the sample review\n",
    "sample_predicted_sentiment_lr = lr_classifier.predict(sample_review_tfidf)\n",
    "sample_sentiment = \"Positive\" if sample_predicted_sentiment_lr[0] == 'positive' else \"Negative\"\n",
    "print(f\"Sentiment Prediction (TF-IDF + Logistic Regression): {sample_sentiment}\")\n",
    "\n",
    "# BERT-based Approach\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "results = nlp(review)\n",
    "predicted_sentiment_bert = results[0]['label']\n",
    "print(f\"Sentiment (BERT): {predicted_sentiment_bert}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Challenge of Interaction\n",
    "\n",
    "- Interacting with Language Models (LLMs) was challenging.\n",
    "- They focused on predicting the next word.\n",
    "- Difficulties in following human instructions.\n",
    "\n",
    "- **Instruction Tuning Unveiled**\n",
    "  - Fine-tuning LLMs to follow human instructions.\n",
    "  - Enhanced interaction and task performance.\n",
    "\n",
    "** Benefits of Instruction Tuning**\n",
    "\n",
    "- Increased accuracy and capabilities of LLMs.\n",
    "- Alignment with human values.\n",
    "- Prevention of undesired or dangerous content.\n",
    "\n",
    "\n",
    "#### The Arrival of ChatGPT\n",
    "\n",
    "- ChatGPT: A milestone in Generative AI.\n",
    "- Reorganized instruction tuning into a dialogue format.\n",
    "- User-friendly interface for AI interaction.\n",
    "\n",
    "<img src=\"./images/GAI2.webp\" width=\"1000\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI’s GPT Models\n",
    "\n",
    "<img src=\"./images/GAI3.webp\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "Task specific\n",
    "\n",
    "<img src=\"./images/GAI4.webp\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "\n",
    "<img src=\"./images/GAI5.webp\" width=\"1000\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google AI and the Pathways Language Model (PaLM)\n",
    "\n",
    "<img src=\"./images/PALM.jpeg\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- Google's largest publicly disclosed model.\n",
    "- PaLM serves as a foundation model.\n",
    "- Used in various Google projects.\n",
    "- Unlocking new capabilities in AI.\n",
    "- PaLM variants scale up to 540 billion parameters.\n",
    "- Trained on 780 billion tokens.\n",
    "- A substantial leap beyond GPT-3.\n",
    "\n",
    "#### Training Data\n",
    "\n",
    "- Self-supervised learning with a diverse text corpus.\n",
    "- Multilingual web pages, books, code repositories, and more.\n",
    "- A rich and comprehensive training dataset.\n",
    "\n",
    "#### PaLM's Performance\n",
    "\n",
    "- PaLM's exceptional few-shot performance.\n",
    "- Outperforming prior larger models like GPT-3.\n",
    "\n",
    "#### PaLM's Training Infrastructure\n",
    "\n",
    "- Training on multiple TPU v4 pods.\n",
    "- Dense decoder-only Transformer model.\n",
    "- Efficient scale training without pipeline parallelism.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepMind’s Chinchilla Model\n",
    "\n",
    "<img src=\"./images/Deepmind.webp\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- DeepMind Founded in 2010.\n",
    "- Acquired by Google in 2014, now a subsidiary of Alphabet Inc.\n",
    "  - DeepMind's pursuit of replicating human short-term memory.\n",
    "  - Creation of a Neural Turing Machine.\n",
    "  - A step towards understanding memory in AI.\n",
    "\n",
    "- AlphaZaro\n",
    "  - Competence achieved through reinforcement learning.\n",
    "- AlphaFold's advances in protein folding.\n",
    "  - Predicting over 200 million protein structures.\n",
    "  - Revolutionizing the field of biology.\n",
    "\n",
    "#### Flamingo - Describing Images\n",
    "\n",
    "- In April 2022, DeepMind launched Flamingo.\n",
    "  - A single visual language model capable of describing any picture.\n",
    "  - Advancing AI's understanding of visual content.\n",
    "  - https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model\n",
    "\n",
    "\n",
    "#### Chinchilla AI - Outperforming GPT-3\n",
    "\n",
    "- DeepMind's Chinchilla AI introduced in March 2022.\n",
    "- Outperforming GPT-3.\n",
    "- How \n",
    "  - Chinchilla boasts 70B parameters.\n",
    "  - Trained on 1,400 tokens, 4.7x more than GPT-3.\n",
    "- Significant benefits for inference costs.\n",
    "  - Outperforming other large language model platforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta AI (formerly FAIR)\n",
    "\n",
    "- FAIR, or Facebook Artificial Intelligence Research.\n",
    "- A laboratory focused on open-source AI frameworks.\n",
    "\n",
    "\n",
    "#### PyText - Advancing NLP\n",
    "\n",
    "- In 2018, FAIR released PyText.\n",
    "- A modeling framework for NLP systems.\n",
    "\n",
    "\n",
    "#### Galactica - Assisting Scientists\n",
    "\n",
    "- November 2022: Meta's Galactica.\n",
    "- Assists scientists with tasks like summarizing papers and annotating molecules.\n",
    "- Bridging the gap between AI and scientific research.\n",
    "\n",
    "\n",
    "#### LLaMA - Large Language Model Meta AI\n",
    "\n",
    "<img src=\"./images/llama.jpeg\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- Released in February 2023.\n",
    "- A foundational transformer-based language model.\n",
    "- Aimed at advancing AI research and academic exploration.\n",
    "- Responsible AI\n",
    "  - LLaMA models released under non-commercial licenses.\n",
    "  - Preventing misuse while promoting responsible AI.\n",
    "  - Access granted to select researchers and organizations.\n",
    "- Parameters\n",
    "  - from 7 billion to 65 billion parameters.\n",
    "  - Comparing LLaMA-65B to Chinchilla and PaLM.\n",
    "- Training Data\n",
    "  - LLaMA models trained on 1.4 trillion tokens in 20 languages.\n",
    "  - Leveraging publicly available unlabeled data.\n",
    "  - Data sources include CCNet, GitHub, Wikipedia, ArXiv, Stack Exchange, and books.\n",
    "\n",
    "- Challenges\n",
    "  - LLaMA's performance varies across languages.\n",
    "  - Challenges related to bias, toxicity, and hallucination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic and the Claude Chatbot\n",
    "\n",
    "<img src=\"./images/claude.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- Anthropic: An AI startup and public benefit corporation.\n",
    "- Founded in 2021 by Daniela Amodei and Dario Amodei, former OpenAI members.\n",
    "- A focus on responsible AI and interpretability.\n",
    "\n",
    "\n",
    "#### Claude Chatbot\n",
    "\n",
    "- Introducing Claude, Anthropic's conversational large language model.\n",
    "- Using **constitutional AI** for better alignment with human intentions.\n",
    "- Claude Models\n",
    "  - Claude comes in two versions: Claude-v1 and Claude Instant.\n",
    "  - Claude-v1 for complex dialogues and creative content.\n",
    "  - Claude Instant for casual conversations and summarization.\n",
    "\n",
    "#### Limitations and Concerns\n",
    "\n",
    "- Claude's limitations in math and programming.\n",
    "- Occasional hallucinations and dubious instructions.\n",
    "- Concerns about clever prompting bypassing safety features.\n",
    "\n",
    "# Availability and Integration\n",
    "\n",
    "- Claude's media embargo lifted in January 2023.\n",
    "- Integration with Discord Juni Tutor Bot and various platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Source Efforts in AI and Machine Learning\n",
    "\n",
    "- The significance of open-source LLMs in AI.\n",
    "- Providing deeper access to LLMs beyond APIs.\n",
    "- The promise of open source in democratizing AI.\n",
    "\n",
    "# Open Source Large Language Models\n",
    "# Language Models Overview\n",
    "\n",
    "| Model Family Name | Created By | Sizes | Focus | Foundation or Fine-Tuned | License | What’s Interesting | Architectural Notes |\n",
    "|-------------------|------------|-------|-------|--------------------------|---------|-------------------|--------------------|\n",
    "| LLaMA | Meta | 7B, 13B, 32B, 65.2B | Varied | Foundation | Non-commercial | Basis for numerous fine-tuned variants | SwiGLU activation instead of ReLU |\n",
    "| LLaMA 2 | Meta with Microsoft | 7B, 13B, 70B | Chat | Foundation | Commercial | Balances safety and helpfulness better than OpenAI's models | SwiGLU activation, RoPE over traditional embeddings |\n",
    "| Alpaca | Stanford’s CRFM | 7B | Instruction following | Fine-tuned LLaMA 7B | Non-commercial | Trained on text-davinci-003 examples | - |\n",
    "| Vicuna | LMSYS | 7B, 13B | Chat | Fine-tuned LLaMA 13B | Non-commercial | Utilizes conversations from ShareGPT.com for training | - |\n",
    "| Guanaco | KBlueLeaf | 7B | Instruction following | Fine-tuned LLaMA 7B (parameter efficient) | Non-commercial | Fine-tuned using QLoRA | - |\n",
    "| RedPajama | Multiple collaborators | 3B, 7B | Chat, Instruction following | Foundation | Commercial | Uses the fully open RedPajama dataset following the LLaMA training recipe | Modifications on the Pythia architecture |\n",
    "| Falcon | Technology Innovation Institute of UAE | 7B, 40B | Varied | Foundation | Commercial | Features a 2D parallelism strategy and ZeRo optimization for efficient training | FlashAttention and Multi-query Attention techniques |\n",
    "| Flan-T5 | Google | Various, up to 11B | Varied | Foundation | Commercial | Trained on a massive collection of datasets, tasks, and task categories | Based on the T5 encoder-decoder structure |\n",
    "| Stable Beluga 2 (Freewilly) | Stability AI | 70B | Varied | Fine-tuned LLaMA 2 70B | Non-commercial | Uses a modified Orca approach for high-quality example generation | - |\n",
    "| MPT | MosaicML | Up to 30B | Varied including story writing | Foundation | Commercial | Capable of generating extremely long texts (up to 84k tokens) with specific configurations | Features FlashAttention |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI applied to other modalities\n",
    "\n",
    "\n",
    "<img src=\"./images/GAI6.png\" width=\"1000\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation: Dall-E | MidJourney | Stable Diffusion | DreamStudio\n",
    "\n",
    "\n",
    "<img src=\"./images/GAI7.webp\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "# AI Innovations in Creativity\n",
    "\n",
    "\n",
    "#### DALL-E\n",
    "\n",
    "**DALL-E** - *By OpenAI*\n",
    "\n",
    "- Generates images from textual descriptions.\n",
    "- Combines \"Dali\" and \"Wall-E.\"\n",
    "- Example: \"a two-story pink house shaped like a shoe.\"\n",
    "- **Limitation:** Limited to predefined concepts, potential for misinterpretation.\n",
    "\n",
    "![DALL-E](dalle_image.jpg)\n",
    "\n",
    "\n",
    "#### Midjourney\n",
    "\n",
    "**Midjourney** - *Artificial Intelligence for Exploration*\n",
    "\n",
    "- Enhances exploration in robotics and space missions.\n",
    "- Facilitates autonomous decision-making.\n",
    "- A step towards AI-driven exploration.\n",
    "- **Limitation:** Dependency on data quality, computational resources.\n",
    "\n",
    "![Midjourney](midjourney_image.jpg)\n",
    "\n",
    "\n",
    "#### Stable Diffusion\n",
    "\n",
    "**Stable Diffusion** - *Generative Model Training*\n",
    "\n",
    "- A technique for training generative models.\n",
    "- Improves stability and quality.\n",
    "- Used in GANs and AI art generation.\n",
    "- **Limitation:** Requires extensive computational power and time.\n",
    "\n",
    "![Stable Diffusion](stable_diffusion_image.jpg)\n",
    "\n",
    "\n",
    "#### DreamStudio\n",
    "\n",
    "**DreamStudio** - *AI-Enhanced Creative Tools*\n",
    "\n",
    "- Empowers artists with AI-generated content.\n",
    "- Seamlessly integrates AI into the creative process.\n",
    "- Enables new forms of artistic expression.\n",
    "- **Limitation:** May raise concerns about AI's role in art creation.\n",
    "\n",
    "![DreamStudio](dreamstudio_image.jpg)\n",
    "\n",
    "\n",
    "# Comparison Table\n",
    "\n",
    "| Innovation     | Description                                | Application                   | Limitation                           | Link                                       |\n",
    "|----------------|--------------------------------------------|--------------------------------|--------------------------------------|--------------------------------------------|\n",
    "| DALL-E         | Generates images from text                | Art, Design, Visual Creativity  | Limited to predefined concepts      | [Link](https://www.openai.com/research/dall-e/) |\n",
    "| Midjourney     | Enhances exploration in robotics           | Space Missions, Autonomous Exploration | Data quality, computational resources | [Link](https://www.example.com/midjourney) |\n",
    "| Stable Diffusion | Training technique for generative models  | Generative Art, GANs          | High computational requirements     | [Link](https://www.example.com/stable-diffusion) |\n",
    "| DreamStudio    | AI-enhanced creative tools                | Visual Arts, Design, Creativity | Ethical/artistic concerns            | [Link](https://www.example.com/dreamstudio) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpaca\n",
    "\n",
    "- **Model Origin**: Fine-tuned from LLaMA 7B\n",
    "- **Training Data**: 52K instruction-following demonstrations\n",
    "- **Comparison**: Similar behavior to OpenAI’s text-davinci-003\n",
    "- **Cost**: <$600 for reproduction\n",
    "- **Code**: [GitHub.com/Stanford-Alpaca/Alpaca7B](#)\n",
    "\n",
    "- Powerful instruction-following models:\n",
    "    - GPT-3.5 (text-davinci-003)\n",
    "    - ChatGPT\n",
    "    - Claude\n",
    "    - Bing Chat\n",
    "- **Challenges**:\n",
    "    - Generation of false information\n",
    "    - Propagation of stereotypes\n",
    "    - Toxic language generation\n",
    "\n",
    "#### Alpaca Model Details\n",
    "- **Purpose**: Addressing deficiencies in instruction-following models\n",
    "- **Base**: Meta’s LLaMA 7B model\n",
    "- **Training Data**: 52K instructions generated using text-davinci-003\n",
    "- **Behavior**: Similar to text-davinci-003\n",
    "- **Cost**: Surprisingly low\n",
    "\n",
    "\n",
    "#### Training Recipe\n",
    "- **Challenges**:\n",
    "    1. Pretrained language model quality\n",
    "    2. High-quality instruction data\n",
    "- **Solution**: Meta’s new LLaMA models & self-instruct method\n",
    "- **Training Details**: Fine-tuned LLaMA 7B on 52K demonstrations from text-davinci-003\n",
    "- **Data Cost**: <$500 using OpenAI API\n",
    "\n",
    "#### Preliminary Evaluation\n",
    "- **Method**: Human evaluation on self-instruct evaluation set\n",
    "- **Comparison**: Blind pairwise comparison between text-davinci-003 & Alpaca 7B\n",
    "- **Results**: Alpaca and text-davinci-003 had very similar performance\n",
    "- **Demo**: Interactive testing of Alpaca model\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./images/alpaca.jpeg\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap: Language Models & LLMs\n",
    "- **Highlight**: Advanced instruction-following models.\n",
    "- **Examples**:\n",
    "    - Alpaca 7B\n",
    "    - ChatGPT \n",
    "    - Claude\n",
    "\n",
    "- **Challenges**: \n",
    "    - Generation of **false information**\n",
    "    - Propagation of **stereotypes**\n",
    "    - Toxic language generation\n",
    "\n",
    "- **Opportunities**: \n",
    "    - Seamless human-computer interaction\n",
    "    - Enhanced content generation\n",
    "    - Automation of complex text-based tasks\n",
    "\n",
    "\n",
    "\n",
    "#### Going back to Generative AI\n",
    "- **Definition**: AI models designed to **produce** new content.\n",
    "- **Scope**: Beyond text! Think images, videos, music, designs.\n",
    "- **Objective**: Generate content nearly indistinguishable from what humans can produce.\n",
    "\n",
    "#### Key Players in Generative AI\n",
    "1. **Generative Adversarial Networks (GANs)**\n",
    "2. **Variational Autoencoders (VAEs)**\n",
    "\n",
    "\n",
    "#### GANs\n",
    "- **Concept**: Duel between two networks.\n",
    "    - **Generator**: Crafts fake data.\n",
    "    - **Discriminator**: Sifts real from fake.\n",
    "- **Training Dynamics**: \n",
    "    - Generator crafts better fakes.\n",
    "    - Discriminator refines its discernment.\n",
    "- **Applications**:\n",
    "    - Art creation (e.g., DeepArt)\n",
    "    - Image super-resolution (e.g., SRGAN)\n",
    "    - Generating faces (e.g., NVIDIA's FaceGAN)\n",
    "\n",
    "#### VAEs\n",
    "- **Philosophy**: Compress data, then rebuild it.\n",
    "- **Key Mechanism**: Introduces randomness during compression.\n",
    "- **Benefits**:\n",
    "    - Structured latent space for generation.\n",
    "    - More consistent generation than GANs.\n",
    "- **Applications**:\n",
    "    - Image denoising\n",
    "    - Content interpolation (e.g., morphing one image into another)\n",
    "    - Generating art with unique styles\n",
    "\n",
    "# GANs vs VAEs: Quick Comparison\n",
    "- **Training Stability**:\n",
    "    - GANs can be trickier to train due to the dueling nature.\n",
    "    - VAEs generally offer stable training but might not achieve the same level of detail as GANs.\n",
    "- **Generated Data Quality**:\n",
    "    - GANs often produce sharper images.\n",
    "    - VAEs might produce smoother or blurrier images.\n",
    "- **Applications**:\n",
    "    - GANs dominate in tasks requiring high-resolution outputs.\n",
    "    - VAEs shine where a structured latent space or consistent output is vital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GANs\n",
    "\n",
    "- Traditional Machine Learning\n",
    "  - Examines a complex input, like an **image**.\n",
    "  - Produces a simple output, like a **label** (\"cat\").\n",
    "\n",
    "- Generative Models\n",
    "  - Takes a small piece of input, perhaps a few random numbers.\n",
    "  - Produces a complex output, like an image of a **realistic-looking face**.\n",
    "\n",
    "- Generative Adversarial Network (GAN)\n",
    "  - An effective type of **generative model**.\n",
    "  - Introduced only a few years ago.\n",
    "\n",
    "- Why Use GANs?\n",
    "  1. **Intellectual Challenge**: Crafting systems that can generate realistic data.\n",
    "  2. **Practical Applications**: From creating art to enhancing blurry images.\n",
    "\n",
    "\n",
    "\n",
    "#### How does a GAN work?\n",
    "\n",
    "- GANs turn the seemingly impossible goal into reality.\n",
    "- They utilize two key tricks.\n",
    "\n",
    "- Using Randomness as an Ingredient\n",
    "\n",
    "    1. **Variety**: Avoids producing the same output each time.\n",
    "    2. **Mathematical Framework**: Translates image generation into probabilities.\n",
    "    3. **Probability Distribution on Images**: Determines which images are likely to be faces.\n",
    "\n",
    "- Neural Networks and Image Generation\n",
    "  - Modeling a function on a high-dimensional space.\n",
    "  - Neural networks excel at this kind of problem.\n",
    "\n",
    "- The Big Insight: Contest!\n",
    "\n",
    "    - The **\"adversarial\"** part of GAN.\n",
    "    - Two competing networks:\n",
    "    1. **Generator**: Creates random synthetic outputs.\n",
    "    2. **Discriminator**: Distinguishes real outputs from synthetic ones.\n",
    "    \n",
    "- The Adversarial Duel\n",
    "  - The two networks face off.\n",
    "  - They improve together, aiming for a generator that creates realistic outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/GANLab.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "- https://poloclub.github.io/ganlab/\n",
    "  - GANs are complex systems.\n",
    "  - This visualization simplifies GAN mechanics for clarity.\n",
    "\n",
    "\n",
    "- Simplified GAN Mechanics\n",
    "\n",
    "  - Instead of realistic images, the focus is on a **distribution of points in 2D**.\n",
    "  - Easier to understand the mechanics in plain old (x,y) space.\n",
    "\n",
    "- Intruction\n",
    "  - Choose a probability distribution for the GAN to learn.\n",
    "  - Visualized as a **set of data samples**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Variational Autoencoder (VAE) to Generate Images\n",
    "\n",
    "\n",
    "- What is an Autoencoder?\n",
    "  - Transforms input to a **lower dimensional space** (encoding step).\n",
    "  - Reconstructs input from the lower-dimensional representation (decoding step).\n",
    "\n",
    "--  Visual Representation\n",
    "\n",
    "<img src=\"./images/VAE1.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- To generate new images using a VAE, input random vectors to the decoder.\n",
    "\n",
    "<img src=\"./images/VAE2.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "\n",
    "- VAE vs Regular Autoencoders\n",
    "\n",
    "  1. Imposes a **probability distribution** on the latent space.\n",
    "  2. Learns the distribution so the outputs match the observed data.\n",
    "  3. Latent outputs are randomly sampled from the distribution learned by the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/border.jpg\" height=\"10\" width=\"1500\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
